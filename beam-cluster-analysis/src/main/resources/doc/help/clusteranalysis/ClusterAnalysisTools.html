<html>
<head>
    <title>Cluster Analysis Tools</title>
    <link rel="stylesheet" href="../style.css">
</head>

<body>

<table class="header">
    <tr class="header">
        <td class="header">&nbsp;
            Cluster Analysis
        </td>
        <td class="header" align="right"><a href="../general/BeamOverview.html"><img src="../images/BeamHeader.jpg"
                                                                                     border=0></a>
        </td>
    </tr>
</table>

<h3>Introduction</h3>

<p>
    <em>Cluster analysis</em> (or <em>clustering</em>) is the classification of objects into different groups, or more
    precisely, the
    partitioning of a data set into subsets (clusters), so that the data in each subset (ideally) share some common
    trait - often proximity according to some defined distance measure. Data clustering is a common technique for
    statistical data analysis, which is used in many fields, including machine learning, data mining, pattern
    recognition, image analysis and bioinformatics. The computational task of classifying the data set into k clusters
    is often referred to as k-clustering.
</p>

<h3>Types of clustering</h3>

<p>
    Data clustering algorithms can be hierarchical. Hierarchical algorithms find successive clusters using previously
    established clusters. Hierarchical algorithms can be agglomerative ("bottom-up") or divisive ("top-down").
    Agglomerative algorithms begin with each element as a separate cluster and merge them into successively larger
    clusters. Divisive algorithms begin with the whole set and proceed to divide it into successively smaller clusters.
    <em>Partitional</em> algorithms typically determine all clusters at once, but can also be used as divisive
    algorithms in the hierarchical clustering.
</p>

<h3>Distance measures</h3>

<p>
    An important step in any clustering is to select a distance measure, which will determine how the similarity of two
    elements is calculated. This will influence the shape of the clusters, as some elements may be close to one another
    according to one distance and further away according to another. For example, in a 2-dimensional space, the distance
    between the point <code>(1, 0)</code> and the origin <code>(0, 0)</code> is always unity according to the usual
    norms, but the distance between the point <code>(1, 1)</code> and the origin can be <code>2</code>, or
    <code>1</code> if you take respectively the 1-norm, 2-norm or infinity-norm distance.
</p>

<p>
    Particularly important distance measures are the <em>Euclidean distance</em> (or 2-norm distance), which probably is
    the most common, and the <em>Mahalanobis</em> distance, which is similar to the Euclidean distance, but corrects
    data for different scales and correlations in the variables.
</p>

<h3>BEAM cluster analysis tools</h3>

<h5><a href="KMeans.html">K-means cluster analysis</a></h5>

<p>
    The <a href="KMeans.html">K-means</a> algorithm assigns each point to the cluster whose center is nearest. The
    center is the average of all points in the cluster Ñ that is, its coordinates are the arithmetic mean for each
    dimension separately over all the points in the cluster.
</p>

<p>
    Given the number of clusters <em>k</em>, the basic algorithm implemented in BEAM is:
</p>
<ol>
    <li>Randomly choose <em>k</em> pixels defining the cluster centers.</li>
    <li>Assign each pixel to the nearest cluster center as defined by the <em>Euclidean distance</em>.</li>
    <li>Recompute each cluster center as the arithmetic mean of the cluster.</li>
    <li>Repeat the last two steps until the convergence criterion (either the ).</li>
</ol>
<p>
    The main advantages of this algorithm are its simplicity and speed which allows it to run on large datasets. Its
    disadvantage is that it does not yield the same result with each run, since the resulting clusters depend on the
    initial random assignments. It minimizes intra-cluster variance, but does not ensure that the result has a global
    minimum of variance.
</p>

<h5>Expectation maximization (EM) cluster analysis</h5>

<h3>Further information</h3>

<p>A good starting point for obtaining further information on cluster analysis terms and algorithms is the <a
        href="http://en.wikipedia.org/wiki/Data_clustering">Wikipedia entry on data clustering</a>.</p>

<br>
<hr>
</body>
</html>
